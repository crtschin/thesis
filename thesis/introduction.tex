\section{Introduction}
% TODO: Work through MV feedback
In machine learning and more specifically neural network research, researchers set up functions between input and output data and through an algorithm called back propagation, try to optimize the network such that it learns how to solve the problem implied by the data.
Back propagation is a specialized version of automatic differentiation, reverse-mode automatic differentiation to be precise, but programming in an environment which allows for automatic differentiation can be limiting.

Frameworks such as Tangent\fancyfootnote{https://github.com/google/tangent} or autograd\fancyfootnote{https://github.com/HIPS/autograd} are define-by-run algorithms, whose main tactic is to build up the derivative calculation dynamically during runtime.
This process can restrict which high-level optimizations one can apply to generated code.
Support for higher-order derivatives is also limited.

Programming language research has a rich history, with many well-known both high and low-level optimization techniques such as partial evaluation and deforestation.
Exposing these optimization techniques to the world of automatic differentiation can be very fruitful as these calculations are very expensive and often require significant computing power to run.
Through other concepts such as higher-order functions and type systems, we would also get additional benefits such as code-reusability and correctness.

In this thesis, we aim to formalize an extensible correctness proof of an implementation of automatic differentiation on a simply-typed lambda calculus in the \<Coq> proof assistant, opening up further possibilities for formally proving the correctness of more complex language features in the future.
Our formalization is based on a recent proof by Huot, Staton, and \Vakar{} \cite{huot2020correctness}.
They proved, using a denotational model of diffeological spaces, that their forward-mode emulating macro is correct when applied to a simply-typed lambda calculus with products, co-products and inductive types.

With this thesis we make the following core contributions:
\begin{itemize}
  \item Formalize the proofs of both the forward-mode and continuation-based automatic differentiation algorithms specified by Huot, Staton, and \Vakar{} \cite{huot2020correctness} in \<Coq>.
  \item Prove the semantic correctness of various useful compile-time optimizations techniques in the context of generating performant code for automatic differentiation.
  \item Extend the proofs with the array types and compile-time optimization rules by Shaikhha, et al.\cite{Shaikha2019}.
  \item Analyze both the requirements of and issues involved with giving a formal proof of correctness for the combinator-based reverse-mode automatic differentiation algorithm by \Vakar{}\cite{vkr2020reverse}.
\end{itemize}

% TODO: Add references
\Cref{sec:bg} includes a background section explaining many of the topics and techniques used in this thesis. The formalization of the ubiquitous forward-mode automatic differentiation is given in \cref{sec:forward}, starting from a base simply-typed lambda calculus extended with product types and incrementally adding new types and language constructs. \Cref{sec:opt,sec:continuation-base} give formalizations of optimization avenues through, respectively, program transformations and a continuation-based automatic differentiation algorithms.
Finally, \cref{sec:rev} gives our attempt at a formal proof of the recent combinator-based reverse-mode automatic differentiation algorithm by \Vakar{}\cite{vkr2020reverse}.

% TODO: Add a more elaborate section for notations
% As a notational convention, we will use specialized notation in the definitions themselves.
% \<Coq> normally requires that pretty printed notations be defined separately from the definitions they reference.
% The letter $\Gamma$ is used for typing contexts while lowercase Greek letters are usually used for types.
